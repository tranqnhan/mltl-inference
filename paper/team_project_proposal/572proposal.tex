\documentclass[12pt]{article}
% \documentclass{elsarticle} %A different option for format styling...

%NOTE: Use of this template is *totally optional* -- it is just provided to make your life easier. If it does not do that, feel free to use your own template. The formatting does not count for points, only the answers in a readable (LaTeX-generated) format count. 

\textwidth 6.5in
\oddsidemargin 0.0in %this is a 1-inch margin
\evensidemargin 1.0in %matching 1-inch margin

\usepackage{amssymb}
\usepackage{alltt}
\usepackage{multicol}
\usepackage{hyperref}
\usepackage{mathrsfs} %for \mathscr{} 
\usepackage{amsthm}
\usepackage{amsmath}
\usepackage{listings}
\usepackage{gensymb} %for \degree
\usepackage{longtable} %for longtabu
\usepackage{hhline} %for double \hline in longtabu
\usepackage{blindtext}

\newtheorem{defin}{Definition}
\newtheorem{intuit}{Intuition}

%Here are the commands included in elsarticle style:
\newtheorem{thm}{Theorem}
\newtheorem{lem}[thm]{Lemma}

\interfootnotelinepenalty=10000

\renewcommand{\phi}{\varphi}
\newcommand{\always}{\Box}
\newcommand{\eventually}{\Diamond}
\newcommand{\calL}{{\cal L}}

\newcommand{\pspic}[2]{\scalebox{#1}{\includegraphics{#2}}}



%Number the Exercises with one counter through multiple sections
\newcounter{ExerciseCounter}
\setcounter{ExerciseCounter}{1} %start counting at 1


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Figure Magic
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\usepackage{epsfig}
\usepackage{float}
\usepackage{subfigure}
\usepackage{wrapfig}
\renewcommand{\topfraction}{.95} %figures can take up at most 95% of the page before being alone
\renewcommand{\bottomfraction}{.99} %figures can take up at most 99% of the page before being alone
\renewcommand{\textfraction}{.1} %at most this this % of page will be text before making figure-only page
\addtolength{\abovecaptionskip}{-3mm}


\begin{document}

\title{\bf
\large COM S 572 : Principles of Artificial Intelligence \\
\large MLTL Inference -- Proposal}

\author{
  Luke Marzen\\
  \texttt{ljmarzen@iastate.edu}
  \and
  Jayaraman, Swaminathan\\
  \texttt{swamjay@iastate.edu}
  \and
  Nhan Tran\\
  \texttt{nhtran@iastate.edu}
  \and
  Zili Wang\\
  \texttt{ziliw1@iastate.edu}
}

\date{March 8, 2024}

\maketitle

% INSTRUCTIONS FROM CANVAS
% Each group submits via Canvas a proposal that contains the title of your project, a brief (a few paragraphs) outline of the project, and a list of group members as well as the role of each member
% <https://canvas.iastate.edu/courses/108080/assignments/2206881>

The objective of this proposed project is to address the problem of learning Mission-time Linear Temporal Logic (MLTL) formulas from sets of example and counterexample traces.
For a fixed positive integer $n$, a trace is a string $w$ over the alphabet $\Sigma = \{0, 1\}$ such that $|w| = mn$, for some nonnegative integer $m$. 
$w$ can be decomposed into $m$ time steps as $w = w_1 w_2 ... w_m$, where each $|w_i| = n$ can be thought of as a particular assignment to $n$ ordered propositional variables at time step $i$.
Then, a MLTL formula $\phi$ either evaluates to true or false over any given trace, and thus defines a language $\mathcal{L}(\phi)$ over $\Sigma$ such that $w \in \mathcal{L}(\phi)$ if and only if $\phi$ evaluates to true on the trace represented by $w$.
$w$ is a \textbf{positive} trace if we would like $w \in \mathcal{L}(\phi)$, and $w$ is a \textbf{negative} trace if the opposite is desired. 

This problem is formally define as follows: Given a set of traces $T = T^+ \cup T^-$ over $n$ variables, learn a MLTL formula $\phi$ such that $T^+ \subseteq \mathcal{L}(\phi)$ and $T^- \cap \mathcal{L}(\phi) = \emptyset$. There is a large corpus of work on learning regular languages, stemming from Angluin's $L^*$ algorithm \cite{ANGLUIN_Lstar} that learns a minimal deterministic finite automata (DFA) from positive and negative examples. Work in learning temporal logic formulas have also recently been explored, with approaches ranging from symbolic learning algorithms \cite{roy_ltlf_learning, camacho_ltlf_learning} to deep learning algorithms \cite{stl_learning, Luo_Liang_Du_Wan_Peng_Zhang_2022}.

To address this problem we aim to implement and evaluate several approaches to this problem.  To our knowledge this will be the first comprehensive comparison of MLTL inference techniques.  Each team member will be responsible for implementing a different technique, though if some techniques are found to be non-viable or require more effort than reasonable to complete in a semester we may have multiple team members working on any given technique.

The approaches we are considering for this evaluation include
\begin{itemize}
    \item Graph Neural Network with weight extraction
    \item Transformer Neural Network
    \item Template driven search
    \item Automata learning
    \item A* search
\end{itemize}

Previously, Zili Wang has implemented a Genetic Evolution model to learn MLTL formulas with some success which will also be included in our evaluation in our final report.

To analyze and evaluate each approach we will use randomly generated or possibly even real world traces.  Metrics of interest include run time, formula accuracy as a percent of correctly classified traces, and formula length.  Additionally, we will evaluate the methods on different length traces and the complexity of the traces' generating formulas.


\newpage
\bibliographystyle{plain} % We choose the "plain" reference style
\bibliography{Citations} % Entries are in the refs.bib file
\end{document}
